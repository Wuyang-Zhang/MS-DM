# MS-DM

The MS-DM model is a pest detection model proposed in the paper  *“[A multi-species pest recognition and counting method based on a density map in the greenhouse](https://www.sciencedirect.com/science/article/abs/pii/S0168169923009420)”（Zhang Z, Rong J, Qi Z, et al. A multi-species pest recognition and counting method based on a density map in the greenhouse[J]. Computers and Electronics in Agriculture, 2024, 217: 108554.）* , published in the journal  *[Computers and Electronics in Agriculture](https://www.sciencedirect.com/journal/computers-and-electronics-in-agriculture)* .

## Abstract

Whiteflies and fruit flies are common pests that adversely affect greenhouse crops, so it is vital to control their numbers promptly. Current research involves setting up artificial monitoring devices with fixed yellow sticky traps (YSTs), for pest surveillance. However, these devices can only be installed in specific environments and additional interventions may impact the actual distribution of greenhouse pests. On the other hand, detection methods, which are the most commonly used, struggle to accurately detect overlapping or closely interacting small insects, which is a common occurrence in the presence of pests. In this paper, we propose a novel solution for the multi-species pest recognition and counting method based on a density map to tackle pest counting in the greenhouse. To reduce interference between different types of objects, we used two annotation maps to represent the distribution of each object separately. Then, two counting branches are designed to count the pests, respectively. Finally, the detected pests are mapped onto the original image to display their spatial positions. In addition, an adaptive sample equalization method is proposed to address the issue of class imbalance in images, and the atrous spatial pyramid pooling (ASPP) structure and the CBAM attention mechanism are added to the fruit fly branches to focus on learning fruit fly features from crowded edges of YSTs. When applied to split images, our method achieved reliable counting performance in counting both whiteflies and fruit flies, with R^2^ scores of 0.973 and 0.972, respectively. This method has been embedded into mobile devices to assist greenhouse workers in monitoring pest populations in the field. The experiments show that the proposed method provides a promising direction for counting multiple small target insects and monitoring pest populations in crowded images. At the same time, it demonstrates the potential of density map methods in various small target detection scenarios.

## News

## Getting  started

### Env

* [CUDA](https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exenetwork) : cuda 10.0+
  * You can test the current CUDA environment with `nvcc -V` command or `nvidia-smi`;

```bash
conda create -n msdm python==3.7 -y
conda activate msdm
pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/cu100/torch_stable.html
```

On Windows, installing very old versions (PyTorch 1.2 / TorchVision 0.4 / CUDA 10.0) with conda can be problematic, since many mirrors and channels have already removed these packages.

As a result, running the command

```bash
conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch
```

will lead to a  **PackagesNotFoundError** .

PyTorch 1.2 only supports up to Python 3.7, and older CUDA wheels must be installed from the official legacy index.

To install the matching version (CUDA 10.0 with the legacy wheel index), use:

```bash
pip install torch==1.2.0 torchvision==0.4.0 \
    -f https://download.pytorch.org/whl/cu100/torch_stable.html
```

**Note:**

* The `-f https://download.pytorch.org/whl/cu100/torch_stable.html` flag points to the historical wheel index for CUDA 10.0 (cu100).
* Torch wheels installed via `pip` already include the corresponding CUDA runtime, so you don’t need to install `cudatoolkit` separately.

### Other dependency packages

```bash
pip install typing_extensions  pillow==6.2.2 scipy>=1.3.0 tensorboardX
```

### Data Preparation

```bash
project-root/
│
├─ data/                        # Data-related
│  ├─ data-used-by-train-val-test/          # Training/validation/test data for one type of task
│  ├─ data-used-by-train-val-test-another/  # Training/validation/test data for another type of task
│  ├─ images/                   # Auxiliary images for training
│  ├─ mats/                     # Auxiliary .mat files generated for training
│  └─ mats-1/                   # Another set of auxiliary .mat files generated for training
│
├─ datasets/                    # Dataset processing
│  ├─ annotation/               # Raw annotations (can be skipped, directly generated from .mat files)
│  └─ imgs/                     # Raw images (can be skipped, directly generated from .mat files)
│
├─ losses/                      # Loss function modules
│
├─ preprocess/                  # Data preprocessing scripts

```

Make sure that the images are segmented into  **868 × 1156** , the same dimensions as the Shanghai dataset.

First, check the dimensions of all images to determine a suitable strategy for segmentation.

```bash
python  tools\folder-images-sizetype.py

'''
======种类有========
0 : (3264, 2448)
1 : (3468, 4624)
2 : (6936, 9248)
3 : (3472, 4624)
4 : (4624, 3472)
5 : (6944, 9248)
6 : (9248, 6944)
'''
```

During training, remove the images without targets and their corresponding txt files!!!

```bash
python tools\Remove_empty_target_image_based_on_point.py
```

Generate the **.mat** files using `data\generate_mat.py` and save them in the `mats` folder under the **data** directory.

```bash
python data\generate_mat.py
```

Create `data\images` corresponding to `datasets\imgs`

```bash
mkdir -p data/images
```

Then, in the `data\data_used_by_train_val_test` folder, create separate `train`, `test`, and `val` folders.

```bash
mkdir -p data/data-used-by-train-val-test/{train,test,val}
```

Generate  the corresponding `train.txt`, `test.txt`, and `val.txt` file:

`python  data\split-train-val-test-name-to-txt.py`

```bash
python preprocess\preprocess_dataset_nwpu.py
```

```python
i = i.strip().split(',')[0] 
```

 因为有的图片路径中包含空格 所以这里以空格分割改为逗号分割

运行之后data\data-used-by-train-val-test\test 没有.npy   ._densitymap.npy

没关系 可以以后再生成  训练只用到 train  val文件中的东西

### Train

Using the pytorch environment;

```bash
python  train.py
```

You may get an `warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")`  error when running train.py.

The error is due to a Python version issue. Python 3.5 supports the upsample function, but Python 3.6 does not.

Ignoring this warning will reduce experimental performance. Simply put, this warning must be corrected.

The solution is to  Modify the model.py file

```python
from torch.nn import functional as F


        # x = F.upsample_bilinear(x, scale_factor=2)
        x = F.interpolate(x, scale_factor=2)
```

```bash
ImportError cannot import name 'PILLOW_VERSION' from 'PIL' (d:\Miniconda3\Miniconda3py38\envs\msdm\lib\site-packages\PIL\__init__.py) File "G:\MS-DM\datasets\crowd.py", line 6, in <module> import torchvision.transforms.functional as F File "G:\MS-DM\train_helper.py", line 11, in <module> from datasets.crowd import Crowd_qnrf, Crowd_nwpu, Crowd_sh File "G:\MS-DM\train.py", line 4, in <module> from train_helper import Trainer ImportError: cannot import name 'PILLOW_VERSION' from 'PIL' (d:\Miniconda3\Miniconda3py38\envs\msdm\lib\site-packages\PIL\__init__.py)
```

This is a typical compatibility issue introduced in  **Pillow 10.0+** : the `PILLOW_VERSION` constant was removed, but legacy code is still trying to use `from PIL import PILLOW_VERSION.`

```bash
pip uninstall -y pillow
pip install "pillow==6.2.2"
```



Use the `model_qnrf.pth` from the original paper as the pre-trained weights to avoid the long training time of starting from scratch.

```
parser.add_argument('--resume', default=r'pretrained_models\model_qnrf.pth', type=str,
                        help='the path of resume training model')
```

### Test

```bash
python preprocess\preprocess_dataset_nwpu-test.py  
```

得到test文件的  .npy  _density_map.npy文件

这个因为**不用** 单独测试 所以不需要运行

修改修炼好的权重路径

```python
parser.add_argument('--model-path', type=str, default=r'',   help='saved model path')
```

修改包含test文件的父文件夹

```python
parser.add_argument('--data-path', type=str,
                    default=r'data\data-used-by-train-val-test',   
                    help='saved model path')
```

密度图结果保存位置

```python
parser.add_argument('--pred-density-map-path', type=str, default=r'predicted_result', help='save predicted density maps when pred-density-map-path is not empty.')

```

run  test.py 文件

如果想生成test文件里的真值文件

运行 preprocess\preprocess_dataset_nwpu-test.py 即可 不需修改路径 但一般情况下不生成test文件了，重新测试为主  把data\split-train-val-test-name-to-txt.py 比例改变即可

## 多张图训练测试拼接

重新测试文件在 test_images-pre-result-full folder里

**直接放进去即可**  ，不需要任何改动 最后可直接在result文件夹里查看效果图

**输入：**

    est_images   : 存放用于测试的图片
    json : 存放对应的json文件

1. 将坐标点转换成txt文件-----------------》test_images-pre-result-full\point2txt

   run  test_images-pre-result-full\json2point-v1.py
2. 分割坐标和图片

   ```
   ------------------------------test_images-pre-result-full\annotation
   -------------------------------test_images-pre-result-full\images
   ```

   run  test_images-pre-result-full\Splitting_images _and_coordinates.py
3. 将测试的图片姓名写入txt文件 方便生成真值图----------test_images-pre-result-full\test_txt

   run test_images-pre-result-full\split-train-val-test-name-to-txt.py
4. 升mat 文件 生成真值图 -------------test_images-pre-result-full\mats

   run test_images-pre-result-full\generate_mat.py
5. 生成真值图

   run test_images-pre-result-full\preprocess_dataset_nwpu-test.py
6. 进行结果测试   ----------------》  test_images-pre-result-full\result

   run test-v1.py

## Visualization

### CBAMB

#### 修改 model文件 加入CMBA 模块 并权重初始化 应该是model.py  或models-v2.py

#### run   D:\download\DM-Count-master\DM-Count-master\Modify-the-weights-save-the-new.py

根据已经训练好的 权重  生成新的权重文件   **"pretrained_models\new_ok_model_qnrf.pth"**

#### 然后使用冻结训练 微调的方法  在train_helper.py or train_helper--v2.py

```python
    def train(self):
        """training process"""
        args = self.args
        for epoch in range(self.start_epoch, args.max_epoch + 1):
            self.logger.info('-' * 5 + 'Epoch {}/{}'.format(epoch, args.max_epoch) + '-' * 5)
            self.epoch = epoch

            #======================add=======================================
            if epoch <= 500:
                # Freeze layers except CBAM module after 500 epochs
                for name, param in self.model.named_parameters():
                    if "cbam" not in name:
                        param.requires_grad = False
                self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=args.lr)
            else:
                # Unfreeze all layers after 500 epochs
                for name, param in self.model.named_parameters():
                    param.requires_grad = True
                self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=args.lr)
            #====================================================================
            self.train_eopch()
            if epoch % args.val_epoch == 0 and epoch >= args.val_start:
                self.val_epoch()
```

## **目录结构详解**

datasets\crowd_test.py   用来test文件 生成数据集

##**相应的修改**##**

### 改了相应的预测回归层  只有一个密度图 但是两个通道  最后分

models_2_desity.py
pretrained_models\new_2_density_model_qnrf.pth
train_helper_2_density.py
train_2_density.py

最后只运行  train_2_density.py 即可

#-----------------------------------------------------------------------------------

### 原来从reg_layer那就开始改的的

models.py
train_helper.py
pretrained_models\new_ok_model_qnrf.pth
train.py
最后只运行  train.py 即可

## 结构目录一览

![image-20230913094521452](image/README/image-20230913094521452.png)

![image-20230913094540221](image/README/image-20230913094540221.png)

## Deployment

### Android

Use ***Android Studio*** to run the[ source code](https://github.com/Wuyang-Zhang/MS_DM_Android) for mobile deployment.；

### Tensorrt
